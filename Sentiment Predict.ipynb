{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11711406,"sourceType":"datasetVersion","datasetId":7351388},{"sourceId":11703433,"sourceType":"datasetVersion","datasetId":7345996}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import thư viện","metadata":{}},{"cell_type":"code","source":"!pip install -Uqq underthesea","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T0bD0c04VuW5","outputId":"0e26245e-13c8-48f5-e009-e8c7db95f175","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T05:17:34.837796Z","iopub.execute_input":"2025-05-07T05:17:34.838426Z","iopub.status.idle":"2025-05-07T05:17:40.737416Z","shell.execute_reply.started":"2025-05-07T05:17:34.838392Z","shell.execute_reply":"2025-05-07T05:17:40.736729Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install torchtext==0.4.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T05:17:40.738809Z","iopub.execute_input":"2025-05-07T05:17:40.739072Z","iopub.status.idle":"2025-05-07T05:18:49.835937Z","shell.execute_reply.started":"2025-05-07T05:17:40.739051Z","shell.execute_reply":"2025-05-07T05:18:49.835223Z"}},"outputs":[{"name":"stdout","text":"Collecting torchtext==0.4.0\n  Downloading torchtext-0.4.0-py3-none-any.whl.metadata (5.0 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.4.0) (4.67.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.4.0) (2.32.3)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torchtext==0.4.0) (2.5.1+cu124)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.4.0) (1.26.4)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from torchtext==0.4.0) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext==0.4.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext==0.4.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext==0.4.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext==0.4.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext==0.4.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext==0.4.0) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.4.0) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.4.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.4.0) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.4.0) (2025.1.31)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.4.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.4.0) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.4.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.4.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.4.0) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.4.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.4.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.4.0) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch->torchtext==0.4.0)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch->torchtext==0.4.0)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch->torchtext==0.4.0)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch->torchtext==0.4.0)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch->torchtext==0.4.0)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch->torchtext==0.4.0)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.4.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.4.0) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch->torchtext==0.4.0)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.4.0) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.4.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torchtext==0.4.0) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torchtext==0.4.0) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchtext==0.4.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchtext==0.4.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchtext==0.4.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchtext==0.4.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchtext==0.4.0) (2024.2.0)\nDownloading torchtext-0.4.0-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchtext\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchtext-0.4.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install deep_translator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T05:18:49.836795Z","iopub.execute_input":"2025-05-07T05:18:49.836983Z","iopub.status.idle":"2025-05-07T05:18:56.146615Z","shell.execute_reply.started":"2025-05-07T05:18:49.836964Z","shell.execute_reply":"2025-05-07T05:18:56.145903Z"}},"outputs":[{"name":"stdout","text":"Collecting deep_translator\n  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (4.13.3)\nRequirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (2.32.3)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.6)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (4.13.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2025.1.31)\nDownloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: deep_translator\nSuccessfully installed deep_translator-1.11.4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EkIr3n43d4vX","outputId":"ce4a3b9e-279b-4a34-c600-8ecf09402c9c","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T05:18:56.148261Z","iopub.execute_input":"2025-05-07T05:18:56.148496Z","iopub.status.idle":"2025-05-07T05:18:56.152200Z","shell.execute_reply.started":"2025-05-07T05:18:56.148475Z","shell.execute_reply":"2025-05-07T05:18:56.151441Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom itertools import chain\nfrom collections import Counter\nfrom underthesea import word_tokenize\nimport re\nimport unicodedata\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport underthesea\nfrom bs4 import BeautifulSoup\nimport logging\nimport os\nfrom textblob import TextBlob\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport random\n\nimport torchtext.vocab as vocab","metadata":{"id":"klZUtbzlwQBW","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T05:18:56.152901Z","iopub.execute_input":"2025-05-07T05:18:56.153172Z","iopub.status.idle":"2025-05-07T05:19:04.908065Z","shell.execute_reply.started":"2025-05-07T05:18:56.153151Z","shell.execute_reply":"2025-05-07T05:19:04.907298Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Tăng cường dữ liệu \nTăng cường dữ liệu có label là 0 vì dữ liệu quá ít so với các label khác","metadata":{}},{"cell_type":"code","source":"\n# Thiết lập logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Từ điển đồng nghĩa mở rộng\nsynonym_dict = {\n    'lỗi': ['hỏng', 'trục trặc', 'không hoạt động'],\n    'nghiêm trọng': ['đáng kể', 'lớn', 'khủng khiếp', 'trầm trọng'],\n    'nhỏ': ['hạn chế', 'khiêm tốn'],\n    # 'mỏi': ['đau', 'mệt mỏi', 'nhức nhối', 'khó chịu'],\n    'nhiễu': ['lỗi hình', 'sai lệch', 'méo mó', 'biến dạng'],\n    'chậm': ['lỗi thời', 'ùn tắc', 'tệ hại', 'ngắc ngoải', 'hơi chậm', 'trì trệ nhẹ', 'không nhanh lắm'],\n    'không': ['mất', 'từ chối'],\n    'tắt': ['ngắt', 'đột ngột dừng', 'sập'],\n    'thất_vọng': ['tức giận', 'buồn bã'],\n    'kém': ['yếu', 'tệ', 'thấp kém'],\n    # 'ổn': ['khá tốt', 'đạt yêu cầu'],\n    # 'nhanh': ['tốc độ tốt', 'hiệu quả'],\n    # 'tốt': ['đẹp', 'hài lòng'],\n    # 'hơi': ['tương đối', 'có phần', 'kém chút'],\n    'tiếc': ['hơi buồn', 'thất vọng', 'chưa ưng']\n}\n\n# Hàm thay thế từ đồng nghĩa\ndef synonym_replacement(text, prob=0.3):\n    if not text:\n        return text\n    words = text.split()\n    new_words = words.copy()\n    replaced = False\n    for i in range(len(new_words)):\n        if random.random() < prob:\n            for word in synonym_dict.keys():\n                if word in new_words[i]:\n                    new_words[i] = random.choice(synonym_dict.get(word, [new_words[i]]))\n                    replaced = True\n                    break\n    if not replaced:\n        logging.warning(f\"Không có từ nào trong '{text}' được thay thế do không khớp với từ điển đồng nghĩa.\")\n    return ' '.join(new_words)\n\n# Hàm tăng cường dữ liệu chỉ với synonym_replacement\ndef augment_data(data, augment_labels=[0], num_augment=2):\n    augmented_data = []\n    for idx, row in data.iterrows():\n        augmented_data.append({'cleaned_data': row['cleaned_data'], 'label': row['label']})\n        if row['label'] in augment_labels:\n            for _ in range(num_augment):\n                aug_text = synonym_replacement(row['cleaned_data'])\n                augmented_data.append({'cleaned_data': aug_text, 'label': row['label']})\n    return pd.DataFrame(augmented_data)\n\n# Hàm chính\ndef main(input_file, output_file='augmented_data.xlsx'):\n    try:\n        data = pd.read_excel(input_file)\n        logging.info(f\"Đọc thành công dữ liệu với {len(data)} mẫu.\")\n    except Exception as e:\n        logging.error(f\"Lỗi khi đọc tệp Excel: {e}\")\n        return\n    \n    augmented_data = augment_data(data, augment_labels=[0], num_augment=2)\n    logging.info(f\"Tổng số mẫu sau khi tăng cường: {len(augmented_data)}\")\n    \n    try:\n        augmented_data.to_excel(output_file, index=False)\n        logging.info(f\"Lưu dữ liệu đã tăng cường vào {output_file}\")\n    except Exception as e:\n        logging.error(f\"Lỗi khi lưu tệp: {e}\")\n\nif __name__ == \"__main__\":\n    input_file = '/kaggle/input/final-data/final_data.xlsx'\n    output_file = 'augmented_data.xlsx'\n    main(input_file, output_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T05:19:04.908906Z","iopub.execute_input":"2025-05-07T05:19:04.909292Z","iopub.status.idle":"2025-05-07T05:19:10.925686Z","shell.execute_reply.started":"2025-05-07T05:19:04.909272Z","shell.execute_reply":"2025-05-07T05:19:10.925109Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"data = pd.read_excel(\"/kaggle/working/augmented_data.xlsx\")\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T05:19:10.926485Z","iopub.execute_input":"2025-05-07T05:19:10.926702Z","iopub.status.idle":"2025-05-07T05:19:12.163242Z","shell.execute_reply.started":"2025-05-07T05:19:10.926686Z","shell.execute_reply":"2025-05-07T05:19:12.162660Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                        cleaned_data  label\n0  shop giao sản_phẩm sử_dụng không hài_lòng sản_...      0\n1  shop giao sản_phẩm sử_dụng không hài_lòng sản_...      0\n2  shop giao sản_phẩm sử_dụng không hài_lòng sản_...      0\n3  đóng_gói cẩn_thận sp nhỏ gọn hài_lòng cám_ơn tiki      1\n4                   sản_phẩm nhỏ gọn giao hàng nhanh      2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cleaned_data</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>shop giao sản_phẩm sử_dụng không hài_lòng sản_...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>shop giao sản_phẩm sử_dụng không hài_lòng sản_...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>shop giao sản_phẩm sử_dụng không hài_lòng sản_...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>đóng_gói cẩn_thận sp nhỏ gọn hài_lòng cám_ơn tiki</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sản_phẩm nhỏ gọn giao hàng nhanh</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"data['label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T05:19:12.163886Z","iopub.execute_input":"2025-05-07T05:19:12.164078Z","iopub.status.idle":"2025-05-07T05:19:12.174257Z","shell.execute_reply.started":"2025-05-07T05:19:12.164060Z","shell.execute_reply":"2025-05-07T05:19:12.173452Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"label\n2    14907\n0    12222\n1     7925\nName: count, dtype: int64"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"# Load pretrained Vietnamese word2vec\n# Embedding \n# Huấn luyện với mô hình lstm","metadata":{"id":"o7nzAWaPp0No"}},{"cell_type":"code","source":"# Thiết lập logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# 1. Tạo file vi_word2vec.txt từ word2vec_vi_words_100dims.txt\nprint(\"Tạo tệp vi_word2vec.txt...\")\nwith open(\"/kaggle/input/word2vec/word2vec_vi_words_100dims.txt\", \"r\", encoding='utf-8') as f:\n    first_line = f.readline()\n    num_words, dim = [int(word) for word in first_line.split()]\n    with open(\"vi_word2vec.txt\", \"w\", encoding='utf-8') as f_out:\n        for _ in tqdm(range(num_words)):\n            f_out.write(f.readline())\n\n# 2. Tải Word2Vec\nword_embedding = vocab.Vectors(name=\"vi_word2vec.txt\", unk_init=torch.Tensor.normal_)\n\n# 3. Xây dựng Vocabulary\nclass Vocabulary:\n    def __init__(self, min_freq=3):\n        self.word2id = dict()\n        self.word2id['<pad>'] = 0\n        self.word2id['<unk>'] = 1\n        self.unk_id = self.word2id['<unk>']\n        self.id2word = {v: k for k, v in self.word2id.items()}\n        self.min_freq = min_freq\n\n    def __getitem__(self, word):\n        return self.word2id.get(word, self.unk_id)\n\n    def __contains__(self, word):\n        return word in self.word2id\n\n    def __len__(self):\n        return len(self.word2id)\n\n    def add(self, word):\n        if word not in self:\n            word_index = self.word2id[word] = len(self.word2id)\n            self.id2word[word_index] = word\n            return word_index\n        return self[word]\n\n    @staticmethod\n    def tokenize_corpus(corpus):\n        print(\"Tokenize the corpus...\")\n        tokenized_corpus = list()\n        for document in tqdm(corpus):\n            if isinstance(document, str) and document.strip():\n                tokenized_document = word_tokenize(document)\n                tokenized_corpus.append(tokenized_document)\n            else:\n                tokenized_corpus.append([])\n        return tokenized_corpus\n\n    def build_vocab(self, tokenized_corpus):\n        print(\"Building vocabulary...\")\n        word_counts = {}\n        for document in tokenized_corpus:\n            for word in document:\n                word_counts[word] = word_counts.get(word, 0) + 1\n        \n        for word, freq in word_counts.items():\n            if freq >= self.min_freq:\n                self.add(word)\n\n    def corpus_to_tensor(self, corpus, is_tokenized=False):\n        if is_tokenized:\n            tokenized_corpus = corpus\n        else:\n            tokenized_corpus = self.tokenize_corpus(corpus)\n        indicies_corpus = list()\n        for document in tqdm(tokenized_corpus):\n            indicies_document = torch.tensor(list(map(lambda word: self[word], document)),\n                                             dtype=torch.int64)\n            indicies_corpus.append(indicies_document)\n        return indicies_corpus\n\n# 4. Tạo Dataset\nclass SentimentDataset(Dataset):\n    def __init__(self, vocab, data, text_column='cleaned_data', label_column='label'):\n        self.vocab = vocab\n        self.pad_idx = vocab[\"<pad>\"]\n\n        self.reviews_list = list(data[text_column].fillna(''))\n        self.labels_list = list(data[label_column])\n\n        self.tokenized_reviews = self.vocab.tokenize_corpus(self.reviews_list)\n        self.vocab.build_vocab(self.tokenized_reviews)\n        self.tensor_data = self.vocab.corpus_to_tensor(self.tokenized_reviews, is_tokenized=True)\n        self.tensor_label = torch.tensor(self.labels_list, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.tensor_data)\n\n    def __getitem__(self, idx):\n        return self.tensor_data[idx], self.tensor_label[idx]\n\n    def collate_fn(self, examples):\n        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n        reviews = [e[0] for e in examples]\n        reviews = torch.nn.utils.rnn.pad_sequence(reviews, batch_first=False, padding_value=self.pad_idx)\n        reviews_lengths = torch.tensor([len(e[0]) for e in examples])\n        labels = torch.tensor([e[1] for e in examples])\n        return {\"reviews\": (reviews, reviews_lengths), \"labels\": labels}\n\n# 5. Tải và chuẩn bị dữ liệu\nprint(\"Đang tải dữ liệu...\")\ndata = pd.read_excel(\"/kaggle/working/augmented_data.xlsx\")\n\n# Kiểm tra phân bố nhãn\nlabel_counts = data['label'].value_counts()\nlogging.info(f\"Phân bố nhãn: {label_counts.to_dict()}\")\n\n# Chia dữ liệu thành tập huấn luyện và kiểm tra\ntrain_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data['label'])\n\n# 6. Tạo vocabulary và dataset\nvocab = Vocabulary(min_freq=3)\ntrain_dataset = SentimentDataset(vocab, train_data)\ntest_dataset = SentimentDataset(vocab, test_data)\n\n# 7. Tạo DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=train_dataset.collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=test_dataset.collate_fn)\n\nprint(f\"Số mẫu huấn luyện: {len(train_dataset)}\")\nprint(f\"Số mẫu kiểm tra: {len(test_dataset)}\")\nprint(f\"Kích thước từ điển: {len(vocab)}\")\nprint(\"\\n=== Xem dữ liệu trong train_loader (sau padding) ===\")\nfor batch in train_loader:\n    reviews, reviews_lengths = batch[\"reviews\"]\n    labels = batch[\"labels\"]\n    print(\"Batch reviews (padded tensor):\")\n    print(reviews)\n    print(\"Độ dài thực tế của mỗi mẫu trong batch:\", reviews_lengths)\n    print(\"Nhãn của batch:\", labels)\n    # Chuyển tensor padded thành văn bản\n    for i in range(reviews.shape[1]):  # Duyệt qua từng mẫu trong batch\n        words = [vocab.id2word[idx.item()] for idx in reviews[:, i] if idx.item() != vocab[\"<pad>\"]]\n        print(f\"  Mẫu {i} trong batch: {' '.join(words)}\")\n    break  # Chỉ hiển thị batch đầu tiên\n\n# 5. Định nghĩa mô hình LSTM\nclass SentimentLSTM(nn.Module):\n    def __init__(self, vocab, pretrained_embedding, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n        super().__init__()\n        self.vocab = vocab\n        vocab_size = len(vocab)\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n\n        # Ánh xạ vector từ pretrained_embedding cho các từ trong vocab\n        self.init_embeddings(pretrained_embedding)\n\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout, batch_first=False)\n        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def init_embeddings(self, pretrained_embedding):\n        embedding_dim = pretrained_embedding.vectors.size(1)\n        vocab_size = len(self.vocab)\n        # Khởi tạo ngẫu nhiên cho tất cả các từ trong vocab\n        self.embedding.weight.data.normal_(mean=0, std=0.1)\n        # Gán vector từ pretrained_embedding cho các từ có trong vocab\n        for word, idx in self.vocab.word2id.items():\n            if word in pretrained_embedding.stoi:\n                self.embedding.weight.data[idx] = pretrained_embedding.vectors[pretrained_embedding.stoi[word]]\n        # Đảm bảo vector của <pad> là 0\n        self.embedding.weight.data[self.vocab[\"<pad>\"]] = torch.zeros(embedding_dim)\n        # Không huấn luyện embedding\n        self.embedding.weight.requires_grad = False\n\n    def forward(self, text, text_lengths):\n        embedded = self.dropout(self.embedding(text))\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, enforce_sorted=False)\n        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n        if self.lstm.bidirectional:\n            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n        else:\n            hidden = hidden[-1,:,:]\n        return self.fc(self.dropout(hidden))\n\n# 6. Hàm huấn luyện và đánh giá\ndef train_epoch(model, data_loader, criterion, optimizer, clip_value=1.0):\n    model.train()\n    epoch_loss = 0\n    epoch_acc = 0\n    total = 0\n    batch_count = 0\n\n    for batch in tqdm(data_loader, desc=\"Training\"):\n        reviews, reviews_lengths = batch[\"reviews\"]\n        labels = batch[\"labels\"]\n\n        optimizer.zero_grad()\n        predictions = model(reviews, reviews_lengths)\n        loss = criterion(predictions, labels)\n\n        # Kiểm tra loss có hợp lệ không\n        if torch.isnan(loss) or torch.isinf(loss):\n            logging.warning(f\"Loss is invalid at batch {batch_count}: {loss.item()}\")\n            continue\n\n        loss.backward()\n\n        # Gradient clipping để tránh gradient explosion\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        _, predicted = torch.max(predictions, 1)\n        total += labels.size(0)\n        epoch_acc += (predicted == labels).sum().item()\n        batch_count += 1\n\n        # In thông tin từng batch (tuỳ chọn)\n        if batch_count % 10 == 0:\n            logging.info(f\"Batch {batch_count}/{len(data_loader)} - Loss: {loss.item():.4f}, Acc: {(predicted == labels).sum().item() / labels.size(0):.4f}\")\n\n    if batch_count == 0:\n        logging.warning(\"No valid batches to train on!\")\n        return 0, 0\n\n    return epoch_loss / batch_count, epoch_acc / total\n\ndef evaluate(model, data_loader, criterion):\n    model.eval()\n    epoch_loss = 0\n    epoch_acc = 0\n    total = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n            reviews, reviews_lengths = batch[\"reviews\"]\n            labels = batch[\"labels\"]\n\n            predictions = model(reviews, reviews_lengths)\n            loss = criterion(predictions, labels)\n\n            epoch_loss += loss.item()\n            _, predicted = torch.max(predictions, 1)\n            total += labels.size(0)\n            epoch_acc += (predicted == labels).sum().item()\n\n            all_preds.extend(predicted.numpy())\n            all_labels.extend(labels.numpy())\n\n    return epoch_loss / len(data_loader), epoch_acc / total, all_preds, all_labels\n\n# Thêm class weights\nfrom sklearn.utils.class_weight import compute_class_weight\nclass_weights = compute_class_weight('balanced', classes=np.array([0, 1, 2]), y=train_data['label'])\n\nclass_weights = torch.tensor([1.0, 2.0, 0.5], dtype=torch.float)\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha if alpha is not None else torch.ones(3)\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n        pt = torch.exp(-ce_loss)\n        focal_loss = (self.alpha[targets] * (1 - pt) ** self.gamma * ce_loss)\n        \n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        return focal_loss\n\n# Sử dụng Focal Loss thay vì CrossEntropyLoss\ncriterion = FocalLoss(gamma=2.0, alpha=torch.tensor([1.0, 2.0, 0.5]))\n# criterion = nn.CrossEntropyLoss(weight=class_weights)\n\n# Khởi tạo mô hình với hyperparameters mới\nmodel = SentimentLSTM(\n    vocab=vocab,\n    pretrained_embedding=word_embedding,\n    embedding_dim=100,\n    hidden_dim=512,\n    output_dim=3,\n    n_layers=3,\n    bidirectional=True,\n    dropout=0.3\n)\n\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n\n# Huấn luyện với early stopping\nbest_test_loss = float('inf')\npatience = 5\ncounter = 0\nn_epochs = 20\n\nprint(\"\\nBắt đầu huấn luyện...\")\nfor epoch in range(n_epochs):\n    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, clip_value=1.0)\n    test_loss, test_acc, test_preds, test_labels = evaluate(model, test_loader, criterion)\n    scheduler.step()\n\n    print(f'Epoch {epoch+1}/{n_epochs}:')\n    print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n    print(f'  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n\n    # Early stopping\n    if test_loss < best_test_loss:\n        best_test_loss = test_loss\n        counter = 0\n        torch.save(model.state_dict(), 'best_model.pt')\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\n# Đánh giá\nprint(\"\\nBáo cáo phân loại trên tập kiểm tra:\")\nprint(classification_report(test_labels, test_preds, target_names=[str(i) for i in range(3)]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T05:19:26.952383Z","iopub.execute_input":"2025-05-07T05:19:26.952677Z","iopub.status.idle":"2025-05-07T09:00:45.221652Z","shell.execute_reply.started":"2025-05-07T05:19:26.952656Z","shell.execute_reply":"2025-05-07T09:00:45.220884Z"}},"outputs":[{"name":"stdout","text":"Tạo tệp vi_word2vec.txt...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1587507/1587507 [00:29<00:00, 53072.77it/s]\n100%|█████████▉| 1587506/1587507 [01:10<00:00, 22647.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Đang tải dữ liệu...\nTokenize the corpus...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28043/28043 [00:23<00:00, 1213.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Building vocabulary...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28043/28043 [00:00<00:00, 54752.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Tokenize the corpus...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7011/7011 [00:05<00:00, 1178.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Building vocabulary...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7011/7011 [00:00<00:00, 110406.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Số mẫu huấn luyện: 28043\nSố mẫu kiểm tra: 7011\nKích thước từ điển: 6103\n\n=== Xem dữ liệu trong train_loader (sau padding) ===\nBatch reviews (padded tensor):\ntensor([[ 197,   10,  134,  ...,  247,  108, 1124],\n        [ 921,  375, 2951,  ...,  165,  431,    0],\n        [ 801,   97,  928,  ...,  246,    0,    0],\n        ...,\n        [ 140,    0,    0,  ...,    0,    0,    0],\n        [ 930,    0,    0,  ...,    0,    0,    0],\n        [2693,    0,    0,  ...,    0,    0,    0]])\nĐộ dài thực tế của mỗi mẫu trong batch: tensor([68, 39, 31, 30, 25, 19, 19, 18, 16, 16, 15, 13, 12, 12, 12, 11, 11, 10,\n        10,  9,  9,  8,  8,  7,  7,  7,  7,  5,  3,  3,  2,  1])\nNhãn của batch: tensor([2, 0, 1, 1, 0, 0, 2, 1, 0, 0, 2, 2, 2, 2, 0, 2, 1, 0, 1, 2, 0, 2, 0, 2,\n        1, 1, 2, 2, 2, 2, 0, 2])\n  Mẫu 0 trong batch: sản_phẩm huawei lúc_nào ngon dùng 2 i 3 3 i 3 mua 7 i tầm trải_nghiệm mang không đắn_đo 7 i không gogle ap bên thứ 3 ứng_dụng như game ch play bù máy cấu_hình hời so với giá hiệu_năng kirin 810 ngang so_sánh cho dễ hiểu ăn_đứt camera không gogle huawei bù cho khách_hàng cấu_hình như phần_cứng cực_kì tốt tầm_giá mua 6 tr đúng 1 món hời\n  Mẫu 1 trong batch: không hài_lòng với sản_phẩm xài 5 thấy không êm chụp hình nhìn đẹp chẳng thấy ngon mua 2 triệu 1 vsmart mà_lại chơi game mượt hơn bắt wifi mạng 3 g thua rẻ_tiền xài đầu xài sam_sung thất_vọng ghê\n  Mẫu 2 trong batch: nhìn hình đẹp đấy hàng không nhìn tai nghe phần nhựa <unk> nút bấm nặng khó bấm xa với tai nghe akg mua năm_ngoái tuy_nhiên giao hàng nhanh chất_âm tai nghe ok cho 2\n  Mẫu 3 trong batch: thua với mấy bố nhân_viên mua tối nay định mua xiaomi kêu mạnh hơn chơi lag_wifi yếu xem <unk> như chơi liên_quân không <unk> hiệu_năng kém không hiểu làm_ăn kiểu khách_hàng\n  Mẫu 4 trong batch: điện_thoại mua 1 tuần dùng một_tí lóng máy 50 mah dùng tí hết dùng lag lóng máy lag buồn bã điện thoại đổi đổi sang redme hơn\n  Mẫu 5 trong batch: bắt wifi kém hơn samsung a10 bù dùng 4 g load mượt_pin 450 tụt nhanh a10 350 chai pin\n  Mẫu 6 trong batch: dây đẹp lắm nha mọi sz đúng màu như đặt đóng_gói cẩn_thận lắm ship xa hàng tốt ủng_hộ dài\n  Mẫu 7 trong batch: giao nhanh đúng màu nhưng_mà tai nghe mất chân_sạc với tín_hiệu <unk> bên không giá mua nghe vớ vẩn\n  Mẫu 8 trong batch: tgd không tin_tưởng hệ_thống bán_lẻ điện_thoại việt nam mua cửa_hàng thất_vọng như lừa buồn bã vô_cùng chuyển hệ_thống\n  Mẫu 9 trong batch: dùng chán lấy chưa tuần lỗi mở khóa mặt sạc không nhanh lắm pin hết nhanh\n  Mẫu 10 trong batch: mua chìu hôm_qua sp ok máy ngón pin trâu <unk> chưa <unk> cửa_hàng chưa giá\n  Mẫu 11 trong batch: tìm_hiểu kỹ cách <unk> nghe 2 tai sản_phẩm tốt_đẹp loa mong dùng lâu\n  Mẫu 12 trong batch: shop giao hàng nhanh đóng_gói sản_phẩm cẩn_thận hàng y_hình chất lương tốt\n  Mẫu 13 trong batch: hàng đúng mô_tả shop mẫu da to tay m bé pas nhắn_tin\n  Mẫu 14 trong batch: điện_thoại tệ mua giật lag tung chảo mặc_dù không hề cài ứng_dụng\n  Mẫu 15 trong batch: dán phụ kiện đẹp ổn dùng bảo_vệ sạc tai nghe tốt\n  Mẫu 16 trong batch: máy mượt ngon tầm giá khuyên mọi mua nhân_viên tư_vấn nhiệt_tình\n  Mẫu 17 trong batch: sản_phẩm càng_ngày_càng nặng_nề bắt_đc mất wifi tốc_độ cùi máy lỗi\n  Mẫu 18 trong batch: hài_lòng sản_phẩm đặt_hàng buổi tối chiều hôm hàng sử_dụng ok\n  Mẫu 19 trong batch: tuyệt_vời pin bền_hiệu_năng nói_chung la tốt tiền promax tuyệt_vời hơn\n  Mẫu 20 trong batch: bảo mật tệ khuôn_mặt mở đx video quay bảo_mật đỉnh_cao\n  Mẫu 21 trong batch: sản_phẩm dùng tốt_pin trâu cảm_biến vân tay cảm_ơn shop\n  Mẫu 22 trong batch: hỏng nút bấm nhỏ không lên_đèn 1 bên\n  Mẫu 23 trong batch: giá rẻ chất_lượng tốt ủng_hộ tiếp dịp\n  Mẫu 24 trong batch: ổn với mức giá tốt không nổi\n  Mẫu 25 trong batch: hàng <unk> chất_lượng khỏi bàn cảm_ơn tiki\n  Mẫu 26 trong batch: chất_lượng sản_phẩm tuyệt_vời đóng_gói sản_phẩm đẹp chắc_chắn\n  Mẫu 27 trong batch: chất_lượng tốt đường mạng ổn_định\n  Mẫu 28 trong batch: ốp cực_kì đẹp\n  Mẫu 29 trong batch: ok giao nhanh\n  Mẫu 30 trong batch: máy nặng\n  Mẫu 31 trong batch: god\n\nBắt đầu huấn luyện...\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 877/877 [13:00<00:00,  1.12it/s]\nEvaluating: 100%|██████████| 220/220 [00:45<00:00,  4.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20:\n  Train Loss: 0.3526, Train Acc: 0.5180\n  Test Loss: 0.3006, Test Acc: 0.5665\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 877/877 [12:59<00:00,  1.13it/s]\nEvaluating: 100%|██████████| 220/220 [00:47<00:00,  4.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/20:\n  Train Loss: 0.3019, Train Acc: 0.5802\n  Test Loss: 0.2725, Test Acc: 0.6193\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 877/877 [13:06<00:00,  1.12it/s]\nEvaluating: 100%|██████████| 220/220 [00:48<00:00,  4.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/20:\n  Train Loss: 0.2777, Train Acc: 0.6121\n  Test Loss: 0.2759, Test Acc: 0.6523\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 877/877 [13:10<00:00,  1.11it/s]\nEvaluating: 100%|██████████| 220/220 [00:47<00:00,  4.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/20:\n  Train Loss: 0.2608, Train Acc: 0.6225\n  Test Loss: 0.2386, Test Acc: 0.6762\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 877/877 [12:59<00:00,  1.13it/s]\nEvaluating: 100%|██████████| 220/220 [00:46<00:00,  4.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/20:\n  Train Loss: 0.2470, Train Acc: 0.6393\n  Test Loss: 0.2336, Test Acc: 0.6873\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 877/877 [13:04<00:00,  1.12it/s]\nEvaluating: 100%|██████████| 220/220 [00:47<00:00,  4.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/20:\n  Train Loss: 0.2217, Train Acc: 0.6653\n  Test Loss: 0.2160, Test Acc: 0.6834\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 877/877 [13:16<00:00,  1.10it/s]\nEvaluating: 100%|██████████| 220/220 [00:48<00:00,  4.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/20:\n  Train Loss: 0.2105, Train Acc: 0.6752\n  Test Loss: 0.2106, Test Acc: 0.6661\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 877/877 [13:08<00:00,  1.11it/s]\nEvaluating: 100%|██████████| 220/220 [00:47<00:00,  4.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/20:\n  Train Loss: 0.2000, Train Acc: 0.6838\n  Test Loss: 0.2036, Test Acc: 0.7060\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 877/877 [12:37<00:00,  1.16it/s]\nEvaluating: 100%|██████████| 220/220 [00:47<00:00,  4.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/20:\n  Train Loss: 0.1909, Train Acc: 0.6976\n  Test Loss: 0.1976, Test Acc: 0.7040\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 877/877 [12:56<00:00,  1.13it/s]\nEvaluating: 100%|██████████| 220/220 [00:46<00:00,  4.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/20:\n  Train Loss: 0.1814, Train Acc: 0.7038\n  Test Loss: 0.1921, Test Acc: 0.6875\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 877/877 [12:38<00:00,  1.16it/s]\nEvaluating: 100%|██████████| 220/220 [00:46<00:00,  4.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/20:\n  Train Loss: 0.1646, Train Acc: 0.7242\n  Test Loss: 0.1881, Test Acc: 0.7116\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 877/877 [12:38<00:00,  1.16it/s]\nEvaluating: 100%|██████████| 220/220 [00:51<00:00,  4.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/20:\n  Train Loss: 0.1539, Train Acc: 0.7363\n  Test Loss: 0.1921, Test Acc: 0.7194\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 877/877 [12:58<00:00,  1.13it/s]\nEvaluating: 100%|██████████| 220/220 [00:49<00:00,  4.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/20:\n  Train Loss: 0.1497, Train Acc: 0.7441\n  Test Loss: 0.1935, Test Acc: 0.7230\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 877/877 [12:30<00:00,  1.17it/s]\nEvaluating: 100%|██████████| 220/220 [00:48<00:00,  4.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/20:\n  Train Loss: 0.1413, Train Acc: 0.7535\n  Test Loss: 0.1934, Test Acc: 0.7413\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 877/877 [12:41<00:00,  1.15it/s]\nEvaluating: 100%|██████████| 220/220 [00:46<00:00,  4.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/20:\n  Train Loss: 0.1380, Train Acc: 0.7603\n  Test Loss: 0.2106, Test Acc: 0.7364\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 877/877 [12:30<00:00,  1.17it/s]\nEvaluating: 100%|██████████| 220/220 [00:44<00:00,  4.94it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 16/20:\n  Train Loss: 0.1250, Train Acc: 0.7745\n  Test Loss: 0.2052, Test Acc: 0.7401\nEarly stopping triggered!\n\nBáo cáo phân loại trên tập kiểm tra:\n              precision    recall  f1-score   support\n\n           0       0.86      0.92      0.89      2444\n           1       0.47      0.67      0.55      1585\n           2       0.88      0.63      0.73      2982\n\n    accuracy                           0.74      7011\n   macro avg       0.74      0.74      0.73      7011\nweighted avg       0.78      0.74      0.75      7011\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"id":"UNbkwi6C4389"},"outputs":[],"execution_count":null}]}